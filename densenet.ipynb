{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import os\n","import pandas as pd\n","import torch\n","import torchvision.io as io\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","\n","from torchvision import transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["labels_df = pd.read_csv('/kaggle/input/bhw-1-dl-2024-2025/bhw1/labels.csv')\n","categories = sorted(labels_df['Category'].unique())\n","n_classes = len(categories)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["category_to_idx = {category: idx for idx, category in enumerate(categories)}\n","idx_to_category = {idx: category for category, idx in category_to_idx.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def conv_block(in_channels, out_channels):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n","        nn.BatchNorm2d(out_channels),\n","        nn.ReLU(inplace=True)\n","    )\n","\n","class DenseNet(nn.Module):\n","    def __init__(self, num_classes):\n","        super(DenseNet, self).__init__()\n","        \n","        self.conv1 = nn.Sequential(\n","            conv_block(3, 32),\n","            conv_block(32, 64),\n","            conv_block(64, 128),\n","            conv_block(128, 256),\n","            conv_block(256, 512),\n","            nn.MaxPool2d(2)\n","        )\n","        \n","        self.conv2 = nn.Sequential(\n","            conv_block(512, 64),\n","            conv_block(64, 128),\n","            conv_block(128, 256),\n","            conv_block(256, 512),\n","            conv_block(512, 1024),\n","            nn.MaxPool2d(2)\n","        )\n","        \n","        self.conv3 = nn.Sequential(\n","            conv_block(3072, 32),\n","            conv_block(32, 128),\n","            conv_block(128, 256),\n","            conv_block(256, 512),\n","            conv_block(512, 1024),\n","            nn.MaxPool2d(2)\n","        )\n","\n","        self.Ñonv4 = nn.Conv2d(13312, 200, kernel_size=1, bias=False)\n","        self.norm = nn.BatchNorm2d(200)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.pool = nn.AdaptiveAvgPool2d(output_size=1)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","\n","        N, C, H, W = x.size()\n","        unfold1 = F.unfold(x, 2, stride=2).view(N, C * 4, H // 2, W // 2)\n","\n","        x = self.conv2(x)\n","        x = torch.cat((x, unfold1), dim=1)\n","        \n","        N, C, H, W = x.size()\n","        unfold2 = F.unfold(x, 2, stride=2).view(N, C * 4, H // 2, W // 2)\n","\n","        x = self.conv3(x)\n","        x = torch.cat((x, unfold2), dim=1)\n","\n","        x = self.conv4(x)\n","        x = self.norm(x)\n","        x = self.relu(x)\n","        x = self.pool(x)\n","\n","        return x.flatten()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model = DenseNet(200).to(device)\n","print(model)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_transform = transforms.Compose([\n","    transforms.RandomHorizontalFlip(p=0.2), \n","    transforms.RandomVerticalFlip(p=0.2),\n","    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 0.5)),\n","    transforms.RandomRotation(degrees=5),\n","    transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)) \n","])\n","\n","val_transform = transforms.Compose([\n","    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n","])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class TrainDataset(Dataset):\n","    def __init__(self, annotations_file, img_dir, transform=None):\n","        self.img_labels = pd.read_csv(annotations_file)\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.img_labels = self.img_labels[self.img_labels['Id'].apply(\n","            lambda x: os.path.isfile(os.path.join(self.img_dir, x)))]\n","        self.img_labels['Category'] = self.img_labels['Category'].map(category_to_idx)\n","\n","    def __len__(self):\n","        return len(self.img_labels)\n","\n","    def __getitem__(self, idx):\n","        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n","        image = io.read_image(img_path).float() / 255.0\n","        label = self.img_labels.iloc[idx, 1]\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, img_dir, transform=None):\n","        self.img_ids = [img for img in os.listdir(img_dir) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n","        self.img_ids.sort()\n","        self.img_dir = img_dir\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.img_ids)\n","\n","    def __getitem__(self, idx):\n","        img_id = self.img_ids[idx]\n","        img_path = os.path.join(self.img_dir, img_id)\n","        image = io.read_image(img_path).float() / 255.0 \n","        if self.transform:\n","            image = self.transform(image)\n","        return image, img_id"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["full_train_dataset = TrainDataset(\n","    annotations_file='/kaggle/input/bhw-1-dl-2024-2025/bhw1/labels.csv', \n","    img_dir='/kaggle/input/bhw-1-dl-2024-2025/bhw1/trainval'\n",")\n","\n","train_size = int(0.8 * len(full_train_dataset))\n","val_size = len(full_train_dataset) - train_size\n","train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["train_dataset = TrainDataset(\n","    annotations_file='/kaggle/input/bhw-1-dl-2024-2025/bhw1/labels.csv', \n","    img_dir='/kaggle/input/bhw-1-dl-2024-2025/bhw1/trainval', \n","    transform=train_transform\n",")\n","\n","val_dataset = TrainDataset(\n","    annotations_file='/kaggle/input/bhw-1-dl-2024-2025/bhw1/labels.csv', \n","    img_dir='/kaggle/input/bhw-1-dl-2024-2025/bhw1/trainval', \n","    transform=val_transform\n",")\n","\n","train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=4)\n","val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=4)\n","\n","test_dataset = TestDataset(img_dir='/kaggle/input/bhw-1-dl-2024-2025/bhw1/test', transform=val_transform)\n","test_loader = DataLoader(test_dataset, batch_size=256, shuffle=False, num_workers=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n","scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["best_val_accuracy = 0.0\n","best_model_state = None\n","\n","num_epochs = 30\n","\n","for epoch in range(num_epochs):\n","    model.train()\n","    running_loss = 0.0\n","    correct = 0\n","    total = 0\n","    loop = tqdm(train_loader, desc=f'epoch [{epoch+1}/{num_epochs}] - train')\n","    for images, labels in loop:\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        \n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        running_loss += loss.item() * images.size(0)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","        loop.set_postfix(loss=loss.item(), accuracy=100 * correct / total)\n","\n","    epoch_loss = running_loss / len(train_dataset)\n","    epoch_acc = 100 * correct / total\n","    print(f'epoch [{epoch+1}/{num_epochs}], loss: {epoch_loss:.4f}, accuracy: {epoch_acc:.2f}%')\n","    \n","    model.eval()\n","    val_running_loss = 0.0\n","    val_correct = 0\n","    val_total = 0\n","\n","    with torch.no_grad():\n","        loop = tqdm(val_loader, desc=f'epoch [{epoch+1}/{num_epochs}] - val')\n","        for images, labels in loop:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            \n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","            \n","            val_running_loss += loss.item() * images.size(0)\n","            _, predicted = torch.max(outputs.data, 1)\n","            val_total += labels.size(0)\n","            val_correct += (predicted == labels).sum().item()\n","            \n","            loop.set_postfix(loss=loss.item(), accuracy=100 * val_correct / val_total)\n","            \n","    val_epoch_loss = val_running_loss / len(val_dataset)\n","    val_epoch_acc = 100 * val_correct / val_total\n","    print(f'val_loss: {val_epoch_loss:.4f}, val_accuracy: {val_epoch_acc:.2f}%')\n","    \n","    if val_epoch_acc > best_val_accuracy:\n","        best_val_accuracy = val_epoch_acc\n","        best_model_state = model.state_dict()\n","        torch.save(best_model_state, 'best_model.pth')\n","        print(f'val_accuracy: {best_val_accuracy:.2f}%')\n","\n","model.load_state_dict(best_model_state)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.eval()\n","test_predictions = []\n","test_image_ids = []\n","\n","with torch.no_grad():\n","    loop = tqdm(test_loader, desc='Test')\n","    for images, img_ids in loop:\n","        images = images.to(device)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        predicted_categories = [idx_to_category[p.item()] for p in predicted]\n","        test_predictions.extend(predicted_categories)\n","        test_image_ids.extend(img_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sample_submission = pd.read_csv('/kaggle/input/bhw-1-dl-2024-2025/bhw1/sample_submission.csv')\n","\n","submission = pd.DataFrame({\n","    'Id': test_image_ids,\n","    'Category': test_predictions\n","})\n","\n","submission = submission.set_index('Id').loc[sample_submission['Id']].reset_index()\n","\n","submission.to_csv('/kaggle/working/submission.csv', index=False)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":10379392,"sourceId":89777,"sourceType":"competition"}],"dockerImageVersionId":30823,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
